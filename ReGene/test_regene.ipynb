{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the ReGene framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import regene_models\n",
    "importlib.reload(regene_models)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a models directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = regene_models.Classifier(latent_dim=latent_dim, num_classes=10, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier and save\n",
    "classifier.train_classifier(trainloader, num_epochs=10, lr=0.001)\n",
    "torch.save(classifier.state_dict(), 'models/classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's test the classifier on a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random indices for test images\n",
    "random_indices = torch.randint(0, len(trainset), (5,))\n",
    "images = torch.stack([trainset[i][0] for i in random_indices])\n",
    "labels = torch.tensor([trainset[i][1] for i in random_indices])\n",
    "\n",
    "# Get predictions\n",
    "classifier.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "    _, predictions = classifier(images)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "# Plot images with true and predicted labels\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(images[i].cpu().squeeze().numpy(), cmap='gray')\n",
    "    plt.title(f'True: {labels[i].item()}\\nPred: {predicted_classes[i].cpu().item()}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also visualise the latent space. This is done by taking the latent representations of 50 training images and plotting them in 2D using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent representations for 50 random training images\n",
    "random_indices = torch.randint(0, len(trainset), (1000,))\n",
    "images = torch.stack([trainset[i][0] for i in random_indices])\n",
    "labels = torch.tensor([trainset[i][1] for i in random_indices])\n",
    "\n",
    "# Get latent representations\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "    latent_reps, _ = classifier(images)\n",
    "    latent_reps = latent_reps.cpu().numpy()\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "latent_2d = tsne.fit_transform(latent_reps)\n",
    "\n",
    "# Plot the 2D latent space\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10')\n",
    "plt.colorbar(scatter, label='Digit Class')\n",
    "plt.title('t-SNE Visualization of Latent Space')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the decoder, and then train it using the classifier's latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = regene_models.Decoder(latent_dim=latent_dim, device=device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.train_decoder(trainloader, classifier, num_epochs=12, lr=0.001)\n",
    "torch.save(decoder.state_dict(), 'models/decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First less visualise some reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 random images from training set\n",
    "dataiter = iter(trainloader)\n",
    "images, _ = next(dataiter)\n",
    "images = images[:10].to(device)\n",
    "\n",
    "# Get reconstructions\n",
    "classifier.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    z, _ = classifier(images)\n",
    "    reconstructed = decoder(z)\n",
    "\n",
    "# Plot original vs reconstructed images\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    # Original images\n",
    "    axes[0,i].imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0,i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0,i].set_title('Original', pad=10)\n",
    "    \n",
    "    # Reconstructed images  \n",
    "    axes[1,i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[1,i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1,i].set_title('Reconstructed', pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training the models with a joint objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import regene_models\n",
    "importlib.reload(regene_models)\n",
    "from regene_models import ClassifierGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha determines how much weight is given to the reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_decoder = regene_models.Decoder(latent_dim=256, device=device)\n",
    "joint_classifier = regene_models.Classifier(latent_dim=latent_dim, num_classes=10, device=device)\n",
    "regene_models.train_joint(joint_classifier, joint_decoder, trainloader, num_epochs=12, lr=0.001, lambda_recon=0.8)\n",
    "\n",
    "# Save models\n",
    "torch.save(joint_decoder.state_dict(), 'models/joint_decoder.pth')\n",
    "torch.save(joint_classifier.state_dict(), 'models/joint_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some test images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "\n",
    "# Get reconstructions\n",
    "with torch.no_grad():\n",
    "    z, _ = joint_classifier(images)\n",
    "    reconstructed = joint_decoder(z)\n",
    "\n",
    "# Plot original vs reconstructed images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    # Original images\n",
    "    axes[0,i].imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[0,i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0,i].set_title('Original', pad=10)\n",
    "    \n",
    "    # Reconstructed images\n",
    "    axes[1,i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[1,i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1,i].set_title('Reconstructed', pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training encoder and classifier separately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we will train the encoder and classifier separately. The encoder is trained to minimise the reconstruction loss, and the classifier is trained to minimise the cross-entropy loss on the enocders latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regene_models import train_autoencoder, train_classifier_only\n",
    "\n",
    "separate_classifier = regene_models.Classifier(latent_dim=latent_dim, num_classes=10, device=device)\n",
    "separate_decoder = regene_models.Decoder(latent_dim=latent_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder and save models\n",
    "train_autoencoder(classifier=separate_classifier, decoder=separate_decoder, train_loader=trainloader, num_epochs=12, lr=0.001)\n",
    "\n",
    "# Save models\n",
    "torch.save(separate_classifier.state_dict(), 'models/separate_classifier.pth')\n",
    "torch.save(separate_decoder.state_dict(), 'models/separate_decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier_only(separate_classifier, trainloader, num_epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "We will now compare the performance of the different models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "classifier_loaded = regene_models.Classifier(latent_dim=latent_dim, num_classes=10, device=device)\n",
    "decoder_loaded = regene_models.Decoder(latent_dim=latent_dim, device=device)\n",
    "classifier_loaded.load_state_dict(torch.load('models/classifier.pth'))\n",
    "decoder_loaded.load_state_dict(torch.load('models/decoder.pth'))\n",
    "\n",
    "joint_classifier_loaded = regene_models.Classifier(latent_dim=latent_dim, num_classes=10, device=device)\n",
    "joint_decoder_loaded = regene_models.Decoder(latent_dim=latent_dim, device=device)\n",
    "joint_classifier_loaded.load_state_dict(torch.load('models/joint_classifier.pth'))\n",
    "joint_decoder_loaded.load_state_dict(torch.load('models/joint_decoder.pth'))\n",
    "\n",
    "separate_classifier_loaded = regene_models.Classifier(latent_dim=latent_dim, num_classes=10, device=device)\n",
    "separate_decoder_loaded = regene_models.Decoder(latent_dim=latent_dim, device=device)\n",
    "separate_classifier_loaded.load_state_dict(torch.load('models/separate_classifier.pth'))\n",
    "separate_decoder_loaded.load_state_dict(torch.load('models/separate_decoder.pth'))\n",
    "\n",
    "models = [(classifier_loaded, decoder_loaded), (joint_classifier_loaded, joint_decoder_loaded), (separate_classifier_loaded, separate_decoder_loaded)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(classifier, decoder, test_loader):\n",
    "    classifier.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    mse_total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get predictions and reconstructions\n",
    "            z, outputs = classifier(images)\n",
    "            reconstructed = decoder(z)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Calculate MSE\n",
    "            mse = F.mse_loss(reconstructed, images)\n",
    "            mse_total += mse.item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_mse = mse_total / len(test_loader)\n",
    "    \n",
    "    return accuracy, avg_mse\n",
    "\n",
    "# Calculate metrics for each model\n",
    "model_names = ['Standard', 'Joint Training', 'Separate Training']\n",
    "results = []\n",
    "\n",
    "for (clf, dec), name in zip(models, model_names):\n",
    "    accuracy, mse = calculate_metrics(clf, dec, trainloader)\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy (%)': f'{accuracy:.2f}',\n",
    "        'MSE': f'{mse:.4f}'\n",
    "    })\n",
    "\n",
    "# Create and display DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare their latent spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent representations for 1000 random training images\n",
    "random_indices = torch.randint(0, len(trainset), (1000,))\n",
    "images = torch.stack([trainset[i][0] for i in random_indices])\n",
    "labels = torch.tensor([trainset[i][1] for i in random_indices])\n",
    "\n",
    "# Get latent representations for each model\n",
    "model_names = ['Standard', 'Joint Training', 'Separate Training']\n",
    "latent_spaces = []\n",
    "\n",
    "for clf, _ in models:\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        latent_reps, _ = clf(images)\n",
    "        latent_spaces.append(latent_reps.cpu().numpy())\n",
    "\n",
    "# Create subplot for each model's latent space\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('t-SNE Visualization of Latent Spaces')\n",
    "\n",
    "# Perform t-SNE and plot for each model\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "for i, (latent_reps, name) in enumerate(zip(latent_spaces, model_names)):\n",
    "    latent_2d = tsne.fit_transform(latent_reps)\n",
    "    \n",
    "    scatter = axes[i].scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10')\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].set_xlabel('t-SNE Dimension 1')\n",
    "    axes[i].set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(scatter, ax=axes.ravel().tolist(), label='Digit Class')\n",
    "\n",
    "# Add legend to last subplot\n",
    "legend_elements = scatter.legend_elements()[0]\n",
    "axes[-1].legend(legend_elements, range(10), title=\"Classes\", bbox_to_anchor=(1.3, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
