{"cells":[{"cell_type":"markdown","metadata":{"id":"wyWxe51lN7Hb"},"source":["# DDIM Training\n","In this notebook we will train a Denoising Diffusion Implicit Model (DDIM) on the MNIST dataset. We follow the \"Diffusion Autoencoder\" architecture by conditioning the denoising process on the internal representations of a standard autoencoder."]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/Hybrid-CLUE/MyImplementation/DDIM')"],"metadata":{"id":"f2Dy2-5KWoZu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZszWxt0KN7Hc","executionInfo":{"status":"ok","timestamp":1741095241753,"user_tz":0,"elapsed":2273,"user":{"displayName":"Conor Finlay","userId":"06245016195405942659"}},"outputId":"44b992a7-ecdb-499b-9037-420b5afff230"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["# Import the necessary modules\n","import torch\n","import os\n","import matplotlib.pyplot as plt\n","import ddim_mnist\n","import train_ddim_mnist\n","\n","# Add this at the beginning of your second cell (before you load the autoencoder)\n","import importlib\n","importlib.reload(ddim_mnist)\n","from ddim_mnist import DiffusionModel, SimpleAutoencoder\n","\n","importlib.reload(train_ddim_mnist)\n","from train_ddim_mnist import (\n","    train_autoencoder,\n","    prepare_dataset,\n","    train_diffusion_model,\n","    get_device\n",")\n","\n","# Set device (you can use your existing code or the imported function)\n","device = get_device()\n","print(f\"Using device: {device}\")\n","\n","# Create output directory\n","# Assuming your codebase is in 'My Drive/Colab Notebooks/your_project'\n","# Adjust the path if it's different\n","project_path = '/content/drive/My Drive/Hybrid-CLUE/MyImplementation/DDIM'\n","os.makedirs(os.path.join(project_path, \"samples\"), exist_ok=True)"]},{"cell_type":"code","source":["os.chdir(project_path)"],"metadata":{"id":"9qVOhPTfQCvD","executionInfo":{"status":"ok","timestamp":1741095243719,"user_tz":0,"elapsed":1,"user":{"displayName":"Conor Finlay","userId":"06245016195405942659"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"07Yfj5tiN7Hd","executionInfo":{"status":"ok","timestamp":1741096784292,"user_tz":0,"elapsed":1535582,"user":{"displayName":"Conor Finlay","userId":"06245016195405942659"}},"outputId":"0240c9ed-ffcb-4d07-b9d9-b8c3ba58cf2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading existing autoencoder...\n","Autoencoder using device: cuda\n","Autoencoder loaded from autoencoder_weights.pt\n","Generating feature vectors...\n","Creating diffusion model...\n","DiffusionModel using device: cuda\n","Computing dataset statistics...\n","Dataset statistics - Mean: 0.1196, Std: 0.2962\n","Training diffusion model...\n","Epoch: 1/30, Batch: 0/938, Loss: 1.154293\n","Epoch: 1/30, Batch: 100/938, Loss: 0.409515\n","Epoch: 1/30, Batch: 200/938, Loss: 0.277481\n","Epoch: 1/30, Batch: 300/938, Loss: 0.227412\n","Epoch: 1/30, Batch: 400/938, Loss: 0.222889\n","Epoch: 1/30, Batch: 500/938, Loss: 0.191406\n","Epoch: 1/30, Batch: 600/938, Loss: 0.175671\n","Epoch: 1/30, Batch: 700/938, Loss: 0.173009\n","Epoch: 1/30, Batch: 800/938, Loss: 0.166970\n","Epoch: 1/30, Batch: 900/938, Loss: 0.168167\n","Epoch: 1, Train Loss: 0.273817, Val Loss: 0.158668, Time: 51.05s\n","Epoch: 2/30, Batch: 0/938, Loss: 0.158004\n","Epoch: 2/30, Batch: 100/938, Loss: 0.153882\n","Epoch: 2/30, Batch: 200/938, Loss: 0.159149\n","Epoch: 2/30, Batch: 300/938, Loss: 0.147686\n","Epoch: 2/30, Batch: 400/938, Loss: 0.160575\n","Epoch: 2/30, Batch: 500/938, Loss: 0.153780\n","Epoch: 2/30, Batch: 600/938, Loss: 0.142849\n","Epoch: 2/30, Batch: 700/938, Loss: 0.149520\n","Epoch: 2/30, Batch: 800/938, Loss: 0.150287\n","Epoch: 2/30, Batch: 900/938, Loss: 0.140927\n","Epoch: 2, Train Loss: 0.149951, Val Loss: 0.144255, Time: 50.45s\n","Epoch: 3/30, Batch: 0/938, Loss: 0.136482\n","Epoch: 3/30, Batch: 100/938, Loss: 0.134604\n","Epoch: 3/30, Batch: 200/938, Loss: 0.132764\n","Epoch: 3/30, Batch: 300/938, Loss: 0.142059\n","Epoch: 3/30, Batch: 400/938, Loss: 0.141525\n","Epoch: 3/30, Batch: 500/938, Loss: 0.133691\n","Epoch: 3/30, Batch: 600/938, Loss: 0.136174\n","Epoch: 3/30, Batch: 700/938, Loss: 0.140123\n","Epoch: 3/30, Batch: 800/938, Loss: 0.126586\n","Epoch: 3/30, Batch: 900/938, Loss: 0.142400\n","Epoch: 3, Train Loss: 0.139702, Val Loss: 0.135823, Time: 50.75s\n","Epoch: 4/30, Batch: 0/938, Loss: 0.134346\n","Epoch: 4/30, Batch: 100/938, Loss: 0.135766\n","Epoch: 4/30, Batch: 200/938, Loss: 0.133358\n","Epoch: 4/30, Batch: 300/938, Loss: 0.123492\n","Epoch: 4/30, Batch: 400/938, Loss: 0.138457\n","Epoch: 4/30, Batch: 500/938, Loss: 0.125838\n","Epoch: 4/30, Batch: 600/938, Loss: 0.137054\n","Epoch: 4/30, Batch: 700/938, Loss: 0.132562\n","Epoch: 4/30, Batch: 800/938, Loss: 0.134467\n","Epoch: 4/30, Batch: 900/938, Loss: 0.136004\n","Epoch: 4, Train Loss: 0.134832, Val Loss: 0.133690, Time: 50.64s\n","Epoch: 5/30, Batch: 0/938, Loss: 0.132757\n","Epoch: 5/30, Batch: 100/938, Loss: 0.144299\n","Epoch: 5/30, Batch: 200/938, Loss: 0.139512\n","Epoch: 5/30, Batch: 300/938, Loss: 0.133781\n","Epoch: 5/30, Batch: 400/938, Loss: 0.132581\n","Epoch: 5/30, Batch: 500/938, Loss: 0.133513\n","Epoch: 5/30, Batch: 600/938, Loss: 0.124752\n","Epoch: 5/30, Batch: 700/938, Loss: 0.132561\n","Epoch: 5/30, Batch: 800/938, Loss: 0.129758\n","Epoch: 5/30, Batch: 900/938, Loss: 0.137806\n","Epoch: 5, Train Loss: 0.132362, Val Loss: 0.129465, Time: 50.58s\n","Samples saved to samples/epoch_005.png\n","Epoch: 6/30, Batch: 0/938, Loss: 0.128375\n","Epoch: 6/30, Batch: 100/938, Loss: 0.130883\n","Epoch: 6/30, Batch: 200/938, Loss: 0.124129\n","Epoch: 6/30, Batch: 300/938, Loss: 0.123758\n","Epoch: 6/30, Batch: 400/938, Loss: 0.123430\n","Epoch: 6/30, Batch: 500/938, Loss: 0.121215\n","Epoch: 6/30, Batch: 600/938, Loss: 0.124805\n","Epoch: 6/30, Batch: 700/938, Loss: 0.133611\n","Epoch: 6/30, Batch: 800/938, Loss: 0.133096\n","Epoch: 6/30, Batch: 900/938, Loss: 0.124769\n","Epoch: 6, Train Loss: 0.130253, Val Loss: 0.131947, Time: 50.57s\n","Epoch: 7/30, Batch: 0/938, Loss: 0.116566\n","Epoch: 7/30, Batch: 100/938, Loss: 0.135396\n","Epoch: 7/30, Batch: 200/938, Loss: 0.130813\n","Epoch: 7/30, Batch: 300/938, Loss: 0.125734\n","Epoch: 7/30, Batch: 400/938, Loss: 0.126455\n","Epoch: 7/30, Batch: 500/938, Loss: 0.134419\n","Epoch: 7/30, Batch: 600/938, Loss: 0.126513\n","Epoch: 7/30, Batch: 700/938, Loss: 0.134909\n","Epoch: 7/30, Batch: 800/938, Loss: 0.128909\n","Epoch: 7/30, Batch: 900/938, Loss: 0.124162\n","Epoch: 7, Train Loss: 0.128753, Val Loss: 0.128680, Time: 50.34s\n","Epoch: 8/30, Batch: 0/938, Loss: 0.129664\n","Epoch: 8/30, Batch: 100/938, Loss: 0.128033\n","Epoch: 8/30, Batch: 200/938, Loss: 0.132280\n","Epoch: 8/30, Batch: 300/938, Loss: 0.121888\n","Epoch: 8/30, Batch: 400/938, Loss: 0.125663\n","Epoch: 8/30, Batch: 500/938, Loss: 0.134728\n","Epoch: 8/30, Batch: 600/938, Loss: 0.126420\n","Epoch: 8/30, Batch: 700/938, Loss: 0.124134\n","Epoch: 8/30, Batch: 800/938, Loss: 0.125503\n","Epoch: 8/30, Batch: 900/938, Loss: 0.128920\n","Epoch: 8, Train Loss: 0.127637, Val Loss: 0.125745, Time: 50.51s\n","Epoch: 9/30, Batch: 0/938, Loss: 0.124234\n","Epoch: 9/30, Batch: 100/938, Loss: 0.121288\n","Epoch: 9/30, Batch: 200/938, Loss: 0.126221\n","Epoch: 9/30, Batch: 300/938, Loss: 0.123466\n","Epoch: 9/30, Batch: 400/938, Loss: 0.126361\n","Epoch: 9/30, Batch: 500/938, Loss: 0.123899\n","Epoch: 9/30, Batch: 600/938, Loss: 0.121901\n","Epoch: 9/30, Batch: 700/938, Loss: 0.119946\n","Epoch: 9/30, Batch: 800/938, Loss: 0.118948\n","Epoch: 9/30, Batch: 900/938, Loss: 0.132166\n","Epoch: 9, Train Loss: 0.126388, Val Loss: 0.126469, Time: 50.77s\n","Epoch: 10/30, Batch: 0/938, Loss: 0.125913\n","Epoch: 10/30, Batch: 100/938, Loss: 0.134588\n","Epoch: 10/30, Batch: 200/938, Loss: 0.125725\n","Epoch: 10/30, Batch: 300/938, Loss: 0.129919\n","Epoch: 10/30, Batch: 400/938, Loss: 0.127654\n","Epoch: 10/30, Batch: 500/938, Loss: 0.129218\n","Epoch: 10/30, Batch: 600/938, Loss: 0.115107\n","Epoch: 10/30, Batch: 700/938, Loss: 0.121587\n","Epoch: 10/30, Batch: 800/938, Loss: 0.124054\n","Epoch: 10/30, Batch: 900/938, Loss: 0.117527\n","Epoch: 10, Train Loss: 0.125870, Val Loss: 0.124680, Time: 50.79s\n","Samples saved to samples/epoch_010.png\n","Model saved to diffusion_model_epoch_10.pt\n","Epoch: 11/30, Batch: 0/938, Loss: 0.131493\n","Epoch: 11/30, Batch: 100/938, Loss: 0.125449\n","Epoch: 11/30, Batch: 200/938, Loss: 0.124621\n","Epoch: 11/30, Batch: 300/938, Loss: 0.136463\n","Epoch: 11/30, Batch: 400/938, Loss: 0.124779\n","Epoch: 11/30, Batch: 500/938, Loss: 0.124732\n","Epoch: 11/30, Batch: 600/938, Loss: 0.121809\n","Epoch: 11/30, Batch: 700/938, Loss: 0.122430\n","Epoch: 11/30, Batch: 800/938, Loss: 0.122017\n","Epoch: 11/30, Batch: 900/938, Loss: 0.132827\n","Epoch: 11, Train Loss: 0.124971, Val Loss: 0.123296, Time: 50.74s\n","Epoch: 12/30, Batch: 0/938, Loss: 0.123162\n","Epoch: 12/30, Batch: 100/938, Loss: 0.122044\n","Epoch: 12/30, Batch: 200/938, Loss: 0.127794\n","Epoch: 12/30, Batch: 300/938, Loss: 0.118482\n","Epoch: 12/30, Batch: 400/938, Loss: 0.122731\n","Epoch: 12/30, Batch: 500/938, Loss: 0.115253\n","Epoch: 12/30, Batch: 600/938, Loss: 0.122292\n","Epoch: 12/30, Batch: 700/938, Loss: 0.124258\n","Epoch: 12/30, Batch: 800/938, Loss: 0.125771\n","Epoch: 12/30, Batch: 900/938, Loss: 0.120095\n","Epoch: 12, Train Loss: 0.124358, Val Loss: 0.122931, Time: 50.65s\n","Epoch: 13/30, Batch: 0/938, Loss: 0.129686\n","Epoch: 13/30, Batch: 100/938, Loss: 0.119457\n","Epoch: 13/30, Batch: 200/938, Loss: 0.124422\n","Epoch: 13/30, Batch: 300/938, Loss: 0.124580\n","Epoch: 13/30, Batch: 400/938, Loss: 0.132844\n","Epoch: 13/30, Batch: 500/938, Loss: 0.124660\n","Epoch: 13/30, Batch: 600/938, Loss: 0.124301\n","Epoch: 13/30, Batch: 700/938, Loss: 0.120311\n","Epoch: 13/30, Batch: 800/938, Loss: 0.125204\n","Epoch: 13/30, Batch: 900/938, Loss: 0.126241\n","Epoch: 13, Train Loss: 0.123662, Val Loss: 0.122626, Time: 50.55s\n","Epoch: 14/30, Batch: 0/938, Loss: 0.120076\n","Epoch: 14/30, Batch: 100/938, Loss: 0.123660\n","Epoch: 14/30, Batch: 200/938, Loss: 0.119584\n","Epoch: 14/30, Batch: 300/938, Loss: 0.125391\n","Epoch: 14/30, Batch: 400/938, Loss: 0.116089\n","Epoch: 14/30, Batch: 500/938, Loss: 0.121858\n","Epoch: 14/30, Batch: 600/938, Loss: 0.118427\n","Epoch: 14/30, Batch: 700/938, Loss: 0.119959\n","Epoch: 14/30, Batch: 800/938, Loss: 0.123106\n","Epoch: 14/30, Batch: 900/938, Loss: 0.113946\n","Epoch: 14, Train Loss: 0.123305, Val Loss: 0.122377, Time: 50.68s\n","Epoch: 15/30, Batch: 0/938, Loss: 0.125714\n","Epoch: 15/30, Batch: 100/938, Loss: 0.120720\n","Epoch: 15/30, Batch: 200/938, Loss: 0.128087\n","Epoch: 15/30, Batch: 300/938, Loss: 0.124592\n","Epoch: 15/30, Batch: 400/938, Loss: 0.116937\n","Epoch: 15/30, Batch: 500/938, Loss: 0.122874\n","Epoch: 15/30, Batch: 600/938, Loss: 0.114508\n","Epoch: 15/30, Batch: 700/938, Loss: 0.125653\n","Epoch: 15/30, Batch: 800/938, Loss: 0.124401\n","Epoch: 15/30, Batch: 900/938, Loss: 0.118165\n","Epoch: 15, Train Loss: 0.122789, Val Loss: 0.122581, Time: 50.66s\n","Samples saved to samples/epoch_015.png\n","Epoch: 16/30, Batch: 0/938, Loss: 0.129839\n","Epoch: 16/30, Batch: 100/938, Loss: 0.118422\n","Epoch: 16/30, Batch: 200/938, Loss: 0.127357\n","Epoch: 16/30, Batch: 300/938, Loss: 0.122285\n","Epoch: 16/30, Batch: 400/938, Loss: 0.119326\n","Epoch: 16/30, Batch: 500/938, Loss: 0.133389\n","Epoch: 16/30, Batch: 600/938, Loss: 0.123906\n","Epoch: 16/30, Batch: 700/938, Loss: 0.124494\n","Epoch: 16/30, Batch: 800/938, Loss: 0.119338\n","Epoch: 16/30, Batch: 900/938, Loss: 0.120475\n","Epoch: 16, Train Loss: 0.122159, Val Loss: 0.121227, Time: 50.42s\n","Epoch: 17/30, Batch: 0/938, Loss: 0.125730\n","Epoch: 17/30, Batch: 100/938, Loss: 0.119027\n","Epoch: 17/30, Batch: 200/938, Loss: 0.109099\n","Epoch: 17/30, Batch: 300/938, Loss: 0.124299\n","Epoch: 17/30, Batch: 400/938, Loss: 0.114677\n","Epoch: 17/30, Batch: 500/938, Loss: 0.117399\n","Epoch: 17/30, Batch: 600/938, Loss: 0.122607\n","Epoch: 17/30, Batch: 700/938, Loss: 0.118926\n","Epoch: 17/30, Batch: 800/938, Loss: 0.116424\n","Epoch: 17/30, Batch: 900/938, Loss: 0.126018\n","Epoch: 17, Train Loss: 0.121980, Val Loss: 0.123322, Time: 50.54s\n","Epoch: 18/30, Batch: 0/938, Loss: 0.120926\n","Epoch: 18/30, Batch: 100/938, Loss: 0.120967\n","Epoch: 18/30, Batch: 200/938, Loss: 0.117116\n","Epoch: 18/30, Batch: 300/938, Loss: 0.120669\n","Epoch: 18/30, Batch: 400/938, Loss: 0.120460\n","Epoch: 18/30, Batch: 500/938, Loss: 0.121265\n","Epoch: 18/30, Batch: 600/938, Loss: 0.119086\n","Epoch: 18/30, Batch: 700/938, Loss: 0.120724\n","Epoch: 18/30, Batch: 800/938, Loss: 0.118852\n","Epoch: 18/30, Batch: 900/938, Loss: 0.125783\n","Epoch: 18, Train Loss: 0.121315, Val Loss: 0.120137, Time: 50.63s\n","Epoch: 19/30, Batch: 0/938, Loss: 0.120914\n","Epoch: 19/30, Batch: 100/938, Loss: 0.116644\n","Epoch: 19/30, Batch: 200/938, Loss: 0.123013\n","Epoch: 19/30, Batch: 300/938, Loss: 0.123546\n","Epoch: 19/30, Batch: 400/938, Loss: 0.121730\n","Epoch: 19/30, Batch: 500/938, Loss: 0.125237\n","Epoch: 19/30, Batch: 600/938, Loss: 0.122121\n","Epoch: 19/30, Batch: 700/938, Loss: 0.122161\n","Epoch: 19/30, Batch: 800/938, Loss: 0.115883\n","Epoch: 19/30, Batch: 900/938, Loss: 0.116136\n","Epoch: 19, Train Loss: 0.121152, Val Loss: 0.121444, Time: 50.68s\n","Epoch: 20/30, Batch: 0/938, Loss: 0.127917\n","Epoch: 20/30, Batch: 100/938, Loss: 0.121081\n","Epoch: 20/30, Batch: 200/938, Loss: 0.123842\n","Epoch: 20/30, Batch: 300/938, Loss: 0.121927\n","Epoch: 20/30, Batch: 400/938, Loss: 0.119863\n","Epoch: 20/30, Batch: 500/938, Loss: 0.125463\n","Epoch: 20/30, Batch: 600/938, Loss: 0.119250\n","Epoch: 20/30, Batch: 700/938, Loss: 0.119765\n","Epoch: 20/30, Batch: 800/938, Loss: 0.124971\n","Epoch: 20/30, Batch: 900/938, Loss: 0.124894\n","Epoch: 20, Train Loss: 0.120822, Val Loss: 0.119803, Time: 50.88s\n","Samples saved to samples/epoch_020.png\n","Model saved to diffusion_model_epoch_20.pt\n","Epoch: 21/30, Batch: 0/938, Loss: 0.123670\n","Epoch: 21/30, Batch: 100/938, Loss: 0.127770\n","Epoch: 21/30, Batch: 200/938, Loss: 0.123252\n","Epoch: 21/30, Batch: 300/938, Loss: 0.126928\n","Epoch: 21/30, Batch: 400/938, Loss: 0.121194\n","Epoch: 21/30, Batch: 500/938, Loss: 0.116174\n","Epoch: 21/30, Batch: 600/938, Loss: 0.119462\n","Epoch: 21/30, Batch: 700/938, Loss: 0.123069\n","Epoch: 21/30, Batch: 800/938, Loss: 0.131124\n","Epoch: 21/30, Batch: 900/938, Loss: 0.120149\n","Epoch: 21, Train Loss: 0.120618, Val Loss: 0.120211, Time: 51.03s\n","Epoch: 22/30, Batch: 0/938, Loss: 0.117427\n","Epoch: 22/30, Batch: 100/938, Loss: 0.121565\n","Epoch: 22/30, Batch: 200/938, Loss: 0.118966\n","Epoch: 22/30, Batch: 300/938, Loss: 0.118000\n","Epoch: 22/30, Batch: 400/938, Loss: 0.120622\n","Epoch: 22/30, Batch: 500/938, Loss: 0.125150\n","Epoch: 22/30, Batch: 600/938, Loss: 0.113262\n","Epoch: 22/30, Batch: 700/938, Loss: 0.116349\n","Epoch: 22/30, Batch: 800/938, Loss: 0.124762\n","Epoch: 22/30, Batch: 900/938, Loss: 0.123597\n","Epoch: 22, Train Loss: 0.120139, Val Loss: 0.121163, Time: 50.69s\n","Epoch: 23/30, Batch: 0/938, Loss: 0.114656\n","Epoch: 23/30, Batch: 100/938, Loss: 0.111383\n","Epoch: 23/30, Batch: 200/938, Loss: 0.118324\n","Epoch: 23/30, Batch: 300/938, Loss: 0.116350\n","Epoch: 23/30, Batch: 400/938, Loss: 0.115451\n","Epoch: 23/30, Batch: 500/938, Loss: 0.114158\n","Epoch: 23/30, Batch: 600/938, Loss: 0.128690\n","Epoch: 23/30, Batch: 700/938, Loss: 0.117114\n","Epoch: 23/30, Batch: 800/938, Loss: 0.115267\n","Epoch: 23/30, Batch: 900/938, Loss: 0.118006\n","Epoch: 23, Train Loss: 0.119967, Val Loss: 0.119256, Time: 50.75s\n","Epoch: 24/30, Batch: 0/938, Loss: 0.114462\n","Epoch: 24/30, Batch: 100/938, Loss: 0.112088\n","Epoch: 24/30, Batch: 200/938, Loss: 0.116264\n","Epoch: 24/30, Batch: 300/938, Loss: 0.122644\n","Epoch: 24/30, Batch: 400/938, Loss: 0.117613\n","Epoch: 24/30, Batch: 500/938, Loss: 0.115910\n","Epoch: 24/30, Batch: 600/938, Loss: 0.119122\n","Epoch: 24/30, Batch: 700/938, Loss: 0.125705\n","Epoch: 24/30, Batch: 800/938, Loss: 0.122914\n","Epoch: 24/30, Batch: 900/938, Loss: 0.116397\n","Epoch: 24, Train Loss: 0.119807, Val Loss: 0.119782, Time: 50.78s\n","Epoch: 25/30, Batch: 0/938, Loss: 0.114066\n","Epoch: 25/30, Batch: 100/938, Loss: 0.119503\n","Epoch: 25/30, Batch: 200/938, Loss: 0.122944\n","Epoch: 25/30, Batch: 300/938, Loss: 0.114283\n","Epoch: 25/30, Batch: 400/938, Loss: 0.119281\n","Epoch: 25/30, Batch: 500/938, Loss: 0.121141\n","Epoch: 25/30, Batch: 600/938, Loss: 0.116754\n","Epoch: 25/30, Batch: 700/938, Loss: 0.118694\n","Epoch: 25/30, Batch: 800/938, Loss: 0.119832\n","Epoch: 25/30, Batch: 900/938, Loss: 0.117170\n","Epoch: 25, Train Loss: 0.119371, Val Loss: 0.119219, Time: 50.64s\n","Samples saved to samples/epoch_025.png\n","Epoch: 26/30, Batch: 0/938, Loss: 0.111040\n","Epoch: 26/30, Batch: 100/938, Loss: 0.120393\n","Epoch: 26/30, Batch: 200/938, Loss: 0.119989\n","Epoch: 26/30, Batch: 300/938, Loss: 0.126217\n","Epoch: 26/30, Batch: 400/938, Loss: 0.128056\n","Epoch: 26/30, Batch: 500/938, Loss: 0.118801\n","Epoch: 26/30, Batch: 600/938, Loss: 0.115263\n","Epoch: 26/30, Batch: 700/938, Loss: 0.120066\n","Epoch: 26/30, Batch: 800/938, Loss: 0.121620\n","Epoch: 26/30, Batch: 900/938, Loss: 0.120367\n","Epoch: 26, Train Loss: 0.119341, Val Loss: 0.119120, Time: 50.70s\n","Epoch: 27/30, Batch: 0/938, Loss: 0.117051\n","Epoch: 27/30, Batch: 100/938, Loss: 0.116153\n","Epoch: 27/30, Batch: 200/938, Loss: 0.116414\n","Epoch: 27/30, Batch: 300/938, Loss: 0.115314\n","Epoch: 27/30, Batch: 400/938, Loss: 0.123742\n","Epoch: 27/30, Batch: 500/938, Loss: 0.118885\n","Epoch: 27/30, Batch: 600/938, Loss: 0.121147\n","Epoch: 27/30, Batch: 700/938, Loss: 0.122718\n","Epoch: 27/30, Batch: 800/938, Loss: 0.102276\n","Epoch: 27/30, Batch: 900/938, Loss: 0.125282\n","Epoch: 27, Train Loss: 0.119077, Val Loss: 0.118558, Time: 50.75s\n","Epoch: 28/30, Batch: 0/938, Loss: 0.112476\n","Epoch: 28/30, Batch: 100/938, Loss: 0.116003\n","Epoch: 28/30, Batch: 200/938, Loss: 0.116793\n","Epoch: 28/30, Batch: 300/938, Loss: 0.127458\n","Epoch: 28/30, Batch: 400/938, Loss: 0.112686\n","Epoch: 28/30, Batch: 500/938, Loss: 0.117954\n","Epoch: 28/30, Batch: 600/938, Loss: 0.118490\n","Epoch: 28/30, Batch: 700/938, Loss: 0.118828\n","Epoch: 28/30, Batch: 800/938, Loss: 0.119699\n","Epoch: 28/30, Batch: 900/938, Loss: 0.117473\n","Epoch: 28, Train Loss: 0.118532, Val Loss: 0.118974, Time: 50.52s\n","Epoch: 29/30, Batch: 0/938, Loss: 0.115834\n","Epoch: 29/30, Batch: 100/938, Loss: 0.115127\n","Epoch: 29/30, Batch: 200/938, Loss: 0.114837\n","Epoch: 29/30, Batch: 300/938, Loss: 0.114687\n","Epoch: 29/30, Batch: 400/938, Loss: 0.120528\n","Epoch: 29/30, Batch: 500/938, Loss: 0.126713\n","Epoch: 29/30, Batch: 600/938, Loss: 0.128359\n","Epoch: 29/30, Batch: 700/938, Loss: 0.116584\n","Epoch: 29/30, Batch: 800/938, Loss: 0.115018\n","Epoch: 29/30, Batch: 900/938, Loss: 0.120525\n","Epoch: 29, Train Loss: 0.118584, Val Loss: 0.119022, Time: 50.56s\n","Epoch: 30/30, Batch: 0/938, Loss: 0.115255\n","Epoch: 30/30, Batch: 100/938, Loss: 0.120291\n","Epoch: 30/30, Batch: 200/938, Loss: 0.116342\n","Epoch: 30/30, Batch: 300/938, Loss: 0.126882\n","Epoch: 30/30, Batch: 400/938, Loss: 0.115575\n","Epoch: 30/30, Batch: 500/938, Loss: 0.114801\n","Epoch: 30/30, Batch: 600/938, Loss: 0.117079\n","Epoch: 30/30, Batch: 700/938, Loss: 0.113711\n","Epoch: 30/30, Batch: 800/938, Loss: 0.113388\n","Epoch: 30/30, Batch: 900/938, Loss: 0.121386\n","Epoch: 30, Train Loss: 0.118379, Val Loss: 0.118035, Time: 51.01s\n","Samples saved to samples/epoch_030.png\n","Model saved to diffusion_model_epoch_30.pt\n","Model saved to diffusion_model.pt\n"]}],"source":["# Train or load autoencoder\n","if os.path.exists(\"autoencoder_weights.pt\"):\n","    print(\"Loading existing autoencoder...\")\n","    autoencoder = SimpleAutoencoder(device=device)\n","    autoencoder.load_checkpoint(\"autoencoder_weights.pt\")\n","else:\n","    # You can customize parameters here\n","    autoencoder = train_autoencoder(device, batch_size=128, epochs=5)\n","\n","# Prepare dataset with feature vectors\n","train_loader, test_loader, test_features = prepare_dataset(autoencoder, device)\n","\n","# Train or load diffusion model\n","if os.path.exists(\"diffusion_model.pt\"):\n","    print(\"Loading existing diffusion model...\")\n","    diffusion_model = DiffusionModel(device=device)\n","    diffusion_model.load_checkpoint(\"diffusion_model.pt\")\n","else:\n","    # You can customize parameters here\n","    diffusion_model = train_diffusion_model(train_loader, test_loader, test_features, device, epochs=30)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}